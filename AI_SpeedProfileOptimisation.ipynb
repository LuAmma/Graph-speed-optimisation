{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter is a computing environment supporting several interpreted programming languages, cell-by-cell execution, and inline output, including rich tables and charts. It is most often paired with python. It is ideal for quick, iterative data exploration and visualization. All tools and libraries used in this short demonstration are free and open-source.\n",
    "\n",
    "The complete code can be run by pressing the following button:\n",
    "\n",
    "<img src=\"img/RunAll.png\" width=800/>\n",
    "\n",
    "In case the contents of a single cell are changed later, the cell alone can be rerun with the following button. Remeber to also rerun cells whose contents are based on what was just changed. (So perhaps even rerun everything)\n",
    "\n",
    "<img src=\"img/RunCell.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Model\n",
    "So far we saw how to learn from track telemetry data using data analysis.  [Link to Jupyter Notebook](https://github.com/LuAmma/sem_racebootcamp)\n",
    "\n",
    "\n",
    "The next step was a mathematical model of the car, using a Graph to optimize the profile the car should drive. The downside is the reliance on precise modelling and quantifying parameters correctly.\n",
    "\n",
    "\n",
    "That's where machine learning comes in, allowing to use meassured data to predict the values needed for optimising the Graph profile.\n",
    "\n",
    "We need to import libaries first. Scikit-learn (sklearn for short) is a libary commonly used in machine learining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as col\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to the physical model we define how strongly the desire to drive fast and the desire to drive efficient are wighted in our optimization. Only the other will be optimized if either is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_time = 0.2\n",
    "factor_efficiency = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_node(\"x0_k0\",v=0) #add start node\n",
    "coordinate_set_dict = {\"x0_k0\": (0,0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the states states car is allowed to be in. For this we define a track profile using waypoints that we then interpolate. Change the waypoints in order to change the Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       s  vmax       vmin  m_above_sea\n",
      "0      0   0.0   0.000000   100.000000\n",
      "1     50  30.0   5.000000   101.000000\n",
      "2    100  50.0  10.000000   101.300000\n",
      "3    150  50.0  11.250000   101.262500\n",
      "4    200  50.0  12.500000   101.225000\n",
      "5    250  50.0  13.750000   101.187500\n",
      "6    300  50.0  15.000000   101.150000\n",
      "7    350  50.0  16.250000   101.112500\n",
      "8    400  50.0  17.500000   101.075000\n",
      "9    450  50.0  18.750000   101.037500\n",
      "10   500  50.0  20.000000   101.000000\n",
      "11   550  50.0  20.000000   101.500000\n",
      "12   600  50.0  20.000000   102.000000\n",
      "13   650  50.0  20.000000   102.333333\n",
      "14   700  50.0  20.000000   102.666667\n",
      "15   750  50.0  20.000000   103.000000\n",
      "16   800  50.0  20.000000   101.500000\n",
      "17   850  50.0  20.000000   100.000000\n",
      "18   900  50.0  18.664887   100.000000\n",
      "19   950  50.0  17.329773   100.000000\n",
      "20  1000  50.0  15.994660   100.000000\n",
      "21  1050  50.0  14.659546   100.000000\n",
      "22  1100  50.0  13.324433   100.000000\n",
      "23  1150  50.0  11.989319   100.000000\n",
      "24  1200  50.0  10.654206   100.000000\n",
      "25  1250  50.0   9.319092   100.000000\n",
      "26  1300  50.0   7.983979   100.000000\n",
      "27  1350  50.0   6.648865   100.000000\n",
      "28  1400  50.0   5.313752   100.000000\n",
      "29  1450  50.0   3.978638   100.000000\n",
      "30  1500  50.0   2.643525   100.000000\n",
      "31  1550  50.0   1.308411   100.000000\n",
      "32  1600   0.0   0.000000   100.000000\n"
     ]
    }
   ],
   "source": [
    "section_length = 50         # [m]     distance resolution\n",
    "velocity_spacing = 2        # [km/h]  velocity resolution of the simulation\n",
    "max_acceleration = 4        # [1]     Unitless integer of how many velocity_spacing steps the car can accelerate or deaccelerte over one section_length\n",
    "\n",
    "# Define the track profile using waypoints \n",
    "# Between waypoints missing the values are obtained with linear interpolation\n",
    "waypoints = [\n",
    "    {\"s\": 0,    \"vmax\": 0,  \"vmin\": 0,  \"m_above_sea\": 100}, # waypoint at start of track needed\n",
    "    {\"s\": 50,   \"vmax\": 30, \"vmin\": 5, \"m_above_sea\": 101},\n",
    "    {\"s\": 100,  \"vmax\": 50, \"vmin\": 10, \"m_above_sea\": 101.3},\n",
    "    {\"s\": 500,  \"vmax\": 50, \"vmin\": 20, \"m_above_sea\": 101},\n",
    "    {\"s\": 600,  \"vmax\": 50, \"vmin\": 20, \"m_above_sea\": 102},\n",
    "    {\"s\": 750,  \"vmax\": 50, \"vmin\": 20, \"m_above_sea\": 103},\n",
    "    {\"s\": 850,  \"vmax\": 50, \"vmin\": 20, \"m_above_sea\": 100},\n",
    "    {\"s\": 1599, \"vmax\": 50, \"vmin\": 0,  \"m_above_sea\": 100}, # allow speed to drop again before the end\n",
    "    {\"s\": 1600, \"vmax\": 0,  \"vmin\": 0 , \"m_above_sea\": 100}  # waypoint at end of track needed\n",
    "]\n",
    "\n",
    "# Create the full track profile out of the waypoints above\n",
    "waypoint_df = pd.DataFrame(waypoints).set_index(\"s\").sort_index()\n",
    "s_range = np.arange(waypoint_df.index.min(), waypoint_df.index.max() + 1, section_length)\n",
    "config = waypoint_df.reindex(waypoint_df.index.union(s_range)).sort_index().interpolate(\"index\").loc[s_range]\n",
    "config = config.reset_index()\n",
    "\n",
    "\n",
    "x_max = len(config.index)-1  # index of the end of the last section\n",
    "\n",
    "# Helper fuction to calculate the Gradient angle inside section x-1\n",
    "def get_gradient_angle(x):\n",
    "    return math.atan( (config.loc[x, \"m_above_sea\"]-config.loc[x-1, \"m_above_sea\"] ) / section_length)   #in rad\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some functions that will later be used are prepared.\n",
    "\n",
    "The first thing is loading the data. The data from all past attempts on the track are imported, following the principle *\"You can't afford not to use avialable data\"*. For this the files are appended behind one another with some extra logic to make sure the counters in the data are correctly increasing. The data is then cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Loading Data\n",
    "    filenames = ['BE_sample_data_1.csv', 'BE_sample_data_2.csv', 'BE_sample_data_3.csv', 'BE_sample_data_4.csv']\n",
    "    \n",
    "    df_list = []\n",
    "    lap_offset = 0\n",
    "    \n",
    "    # make sure that the lap counter is one increasing value\n",
    "    for file in filenames:\n",
    "        temp_df = pd.read_csv(file, sep=',', low_memory=False)\n",
    "        temp_df.columns = [col.lower() for col in temp_df.columns]\n",
    "    \n",
    "        if 'lap_lap' in temp_df.columns:\n",
    "            temp_df['lap_lap'] += lap_offset\n",
    "            lap_offset = temp_df['lap_lap'].max()+1  # Update offset for next file\n",
    "    \n",
    "        df_list.append(temp_df)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df.ffill(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "       \n",
    "    # Cleanup\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.ffill(inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data currently has the form of time series, but what we want are statistical variables stating the behaviour within a section. That's what the next function does. It takes all data, splits them into bins that correspond to one track section and then calculates macroscopic values for each bin.\n",
    "\n",
    "To gain more data, the procedure is then repeated after shifting the bins by a small distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(df):\n",
    "    # Get max lap_dist, rounded up to a multiple of section_length\n",
    "    max_dist = np.ceil(df['lap_dist'].max() / section_length) * section_length\n",
    "    \n",
    "    # Define how data from the bins should be aggregated into quantities\n",
    "    agg_behavior = {\n",
    "        'E_Engine': pd.NamedAgg(column='jm3_netjoule', aggfunc=lambda x: x.max() - x.min()),  # Total Energy\n",
    "        'time': pd.NamedAgg(column='obc_timestamp', aggfunc=lambda x: x.max() - x.min()),     # total time\n",
    "        's': pd.NamedAgg(column='lap_dist', aggfunc='first'),                                 # distance from start of track to start of section\n",
    "        'v1': pd.NamedAgg(column='gps_speed', aggfunc='first'),                               # speed at the start of the section\n",
    "        'v2': pd.NamedAgg(column='gps_speed', aggfunc='last')                                 # speed at the end of the section\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # define 4 shift values, between 0 and section_length\n",
    "    shifts = np.linspace(0, section_length, 4, endpoint=False)\n",
    "    \n",
    "    for shift in shifts:\n",
    "        bins = np.arange(0, max_dist, section_length)+shift  # Create bins that contain data corresponding to a section, shifted to by shift value \n",
    "    \n",
    "        df['bin'] = pd.cut(df['lap_dist'], bins=bins, right=False)\n",
    "    \n",
    "        # Group and aggregate\n",
    "        grouped = df.groupby(['lap_lap', 'bin'], observed=True).agg(**agg_behavior).reset_index()\n",
    "\n",
    "        #transform speed in km/h into m/s\n",
    "        grouped['v1'] = grouped['v1'] / 3.6\n",
    "        grouped['v2'] = grouped['v2'] / 3.6\n",
    "        \n",
    "        results.append(grouped)\n",
    "    \n",
    "    data = pd.concat(results, ignore_index=True).drop(columns = ['lap_lap','bin'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define how the model is to be trained. For this we create a seperate python class. In the constructor `__init__` the model is trained. The exact specification of the model is passed as an argument, this will be discussed later. Only 90% of the data is used for trainig, in order to have 10% of the data to gauge how good the model is.\n",
    "\n",
    "The `predict` function can then be used to predict the time and Energy used for a section. The arguments \"gradient angle\" and \"section length\" are also passed for compatibility with the physical car model, but not used in this model. In fact the evelation of the track is not considered at all in this model, while it was included in the physical model.\n",
    "\n",
    "There is also a `plot` function that we can use to further evaluate the model, but it's not important what it does exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_nn_wide' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImpossibleState\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mAICarModel\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_nn_wide\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mAICarModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAICarModel\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_nn_wide\u001b[49m):\n\u001b[1;32m      6\u001b[0m         df \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m      7\u001b[0m         agg_df \u001b[38;5;241m=\u001b[39m aggregate_data(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_nn_wide' is not defined"
     ]
    }
   ],
   "source": [
    "class ImpossibleState(Exception):\n",
    "    pass\n",
    "\n",
    "class AICarModel:\n",
    "    def __init__(self, model=model_nn_wide):\n",
    "        df = load_data()\n",
    "        agg_df = aggregate_data(df)\n",
    "        self._model = model\n",
    "\n",
    "\n",
    "        X = agg_df[['v1','v2']]          # Features, quantities that are known\n",
    "        y = agg_df[['E_Engine','time']]  # Target, quantities that we want to know\n",
    "\n",
    "        # Split the data (90% training, 10% testing), so we are later able to evaluate the model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        \n",
    "        # Train the model\n",
    "        self._model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test data\n",
    "        y_pred = self._model.predict(X_test)      \n",
    "\n",
    "        # Compare with test data, and calculate indicators of how good the predictions are.\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print(f\"Mean Squared Error: {mse}\")\n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f\"Root Mean Squared Error: {rmse}\")\n",
    "        for i, col in enumerate(y.columns):  # y is agg_df[['E_Engine','time']]\n",
    "            corr = np.corrcoef(y_test.iloc[:, i], y_pred[:, i])[0, 1]\n",
    "            print(f\"Correlation for {col}: {corr}\")\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,gradient_angle, v1, v2, section_length):  \n",
    "        df = pd.DataFrame([[v1,v2]],columns=[\"v1\",\"v2\"])  # Format Data correctly\n",
    "        prediction = self._model.predict(df)\n",
    "        Energy_Motor, time = prediction[0]\n",
    "\n",
    "        # do some basic plausibility checks to combat mispredictions\n",
    "        if time <= 0: # No negative time\n",
    "            raise ImpossibleState(\"Negative time\") \n",
    "        if Energy_Motor <= 0: # No regaining energy\n",
    "            Energy_Motor = 0      \n",
    "        return Energy_Motor, time\n",
    "\n",
    "\n",
    "    \n",
    "    def plot(self):\n",
    "        # Define range of acceleration and velocity\n",
    "        v1_values = np.linspace(0, 50, 50)       # velocity in m/s\n",
    "        v_diff = np.linspace(-5, 10, 20)         # change in velocity in m/s\n",
    "    \n",
    "        # Create a grid of all combinations\n",
    "        v1_grid, v_diff_grid = np.meshgrid(v1_values, v_diff, indexing='ij')\n",
    "        v1_flat = v1_grid.ravel()\n",
    "        v_diff_flat = v_diff_grid.ravel()\n",
    "    \n",
    "        # Create DataFrame for prediction\n",
    "        df_all = pd.DataFrame({\n",
    "            'v1': v1_flat / 3.6,\n",
    "            'v2': (v1_flat + v_diff_flat) / 3.6,\n",
    "        })\n",
    "    \n",
    "        # Predict\n",
    "        prediction = self._model.predict(df_all)\n",
    "        E_total_flat = prediction[:, 0]\n",
    "        time_flat = prediction[:, 1]\n",
    "    \n",
    "        # Reshape to grid\n",
    "        E_matrix = E_total_flat.reshape(len(v1_values), len(v_diff))\n",
    "        time_matrix = time_flat.reshape(len(v1_values), len(v_diff))\n",
    "    \n",
    "        # Plot both using subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "        im0 = axes[0].imshow(\n",
    "            E_matrix.T,\n",
    "            aspect='auto',\n",
    "            origin='lower',\n",
    "            extent=[v1_values.min(), v1_values.max(), v_diff.min(), v_diff.max()],\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        axes[0].set_title(\"E_total for change from v1 to v2\")\n",
    "        axes[0].set_xlabel(\"Velocity v1 [km/h]\")\n",
    "        axes[0].set_ylabel(\"Change in Velocity [km/h]\")\n",
    "        axes[0].set_xticks(np.linspace(v1_values.min(), v1_values.max(), 5))\n",
    "        axes[0].set_yticks(np.linspace(v_diff.min(), v_diff.max(), 6))\n",
    "        fig.colorbar(im0, ax=axes[0], label='E_total')\n",
    "    \n",
    "        im1 = axes[1].imshow(\n",
    "            time_matrix.T,\n",
    "            aspect='auto',\n",
    "            origin='lower',\n",
    "            extent=[v1_values.min(), v1_values.max(), v_diff.min(), v_diff.max()],\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        axes[1].set_title(\"Time for change from v1 to v2\")\n",
    "        axes[1].set_xlabel(\"Velocity v1 [km/h]\")\n",
    "        axes[1].set_ylabel(\"Change in Velocity [m/s]\")\n",
    "        axes[1].set_xticks(np.linspace(v1_values.min(), v1_values.max(), 5))\n",
    "        axes[1].set_yticks(np.linspace(v_diff.min(), v_diff.max(), 6))\n",
    "        fig.colorbar(im1, ax=axes[1], label='Time')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Pipelines are defined. Pipelines are some kind of \"recipe\" of steps that are taken for training models.\n",
    "There are many otions to construct those pipelines. The following options are written there:\n",
    "\n",
    "* Linear: assumes the the car can be modelled by polynomials up to degree 3.\n",
    "* Lasso: assumes the the car can be modelled by polynomials up to degree 4. Additionally forces the model to be simple by using parameter alpha\n",
    "* NN Wide: uses a neural network to model the data. Here three layers are used, with 128, 256 and 128 nodes respecitvely.\n",
    "* NN Deep: uses a neural network to model the data. Here eight layers are used, with something between 4 and 64 nodes per layer.\n",
    "\n",
    "If you want you can take a look at the [sklearn website](https://scikit-learn.org/stable/) and experiment with the parameters. Particularily the effect of the composition of the neural network can be explored.\n",
    "\n",
    "Note that usually the [pytorch](https://pytorch.org/) libary is used for neural networks instead of sklearn, but as sklearn is easier it is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_linear = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear', MultiOutputRegressor(\n",
    "        LinearRegression()\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline_lasso = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=4, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', MultiOutputRegressor(\n",
    "        Lasso(alpha = 1, max_iter = 20000)\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_nn_wide = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MultiOutputRegressor(\n",
    "       MLPRegressor(\n",
    "           hidden_layer_sizes=(128,256,128), \n",
    "           activation='relu', \n",
    "           random_state=42, \n",
    "           max_iter=5000\n",
    "       )\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_nn_deep = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MultiOutputRegressor(\n",
    "       MLPRegressor(\n",
    "           hidden_layer_sizes=(32,16,64,16, 4, 16, 32, 16), \n",
    "           activation='relu', \n",
    "           random_state=42, \n",
    "           max_iter=5000\n",
    "       )\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one of the pipelines is used to initalise the model.\n",
    "\n",
    "In order to find a model and parameter configuration that fits well, we can look at the following indicators:\n",
    "* Low Mean Squared Error and Root Mean Squared Error for the prediction\n",
    "* Large Correlation between correct values and predictions.\n",
    "\n",
    "These quantities are printed while training the model and are shown below after running the code.\n",
    "\n",
    "Sadly simply minimising or maximising those quantities also does not work, as the following picture illustrates. While the red curve in the picture fits the Data better, it will likely do worse in predicting new data points. This phenomenon is called overfitting. Because of this tradeoff the model has to be simple, but still expressive.\n",
    "\n",
    "<img src=\"img/overfitting.png\" width=800/>\n",
    "\n",
    "Luckily we can also use a different approach to check if the model is suitable.\n",
    "\n",
    "Using the `plot` function the energy usage and time for passing a sector is predicted. On the x-axis is the velocity at the start of the section, at the y-axis by how much this velocity changes. The plots can be compared to the expectation of how the car should react.\n",
    "\n",
    "In particular:\n",
    "1. The Energy usage should be the highest when driving fast and accelerating further\n",
    "2. When deaccelerating, the energy usage should be close to zero, as the engine can be switched off in this state\n",
    "3. At high speeds the energy should be higher than at low speeds\n",
    "4. The time should be porportional to the inverse value of the speed, and as such high in the bottom left corner and low everywhere else\n",
    "\n",
    "Additionally things that do not look smooth should be given a closer look. Is there an explanation for the higher values around the magenta line with the question mark, or does that might hint at overfitting?\n",
    "\n",
    "<img src=\"img/PlotInterpretation.png\" width=800/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use physical formulas to simulate the behaviour of the car\n",
    "# model = PhysicalCarModel()\n",
    "\n",
    "\n",
    "# Use a machinelearing model to predict the behaviour of the car\n",
    "# Uncomment to try out different ones\n",
    "# model = AICarModel(pipeline_linear)\n",
    "# model = AICarModel(pipeline_lasso)\n",
    "model = AICarModel(pipeline_nn_wide)\n",
    "# model = AICarModel(pipeline_nn_deep)\n",
    "\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the graph works exactly the same as with the physical car model, the code is unchanged. To be easier to run everything is in a single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in range (1, x_max+1):   \n",
    "    # change vmin and vmax from the track config to discrete boundaries\n",
    "    k_min = int(np.ceil(config.loc[x, \"vmin\"] / velocity_spacing))\n",
    "    k_max = int(np.floor(config.loc[x, \"vmax\"] / velocity_spacing))\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        v = k * velocity_spacing\n",
    "        node_name = f\"x{x}_k{k}\"\n",
    "        G.add_node((node_name), v=v)\n",
    "        coordinate_set_dict[node_name] = (x*section_length, v) # add to dictionary to later print it at correct position\n",
    "        \n",
    "        for kold in range(k-max_acceleration,k+max_acceleration+1):\n",
    "            if G.has_node(f\"x{x-1}_k{kold}\"):\n",
    "                vold = kold*velocity_spacing\n",
    "\n",
    "                gradient_angle = get_gradient_angle(x)\n",
    "                vold_mps       = vold / 3.6\n",
    "                v_mps          = v / 3.6            \n",
    "\n",
    "                try:\n",
    "                    # Use the car model to predict the Energy and time needed to pass the section\n",
    "                    E_Engine, time = model.predict(gradient_angle, vold_mps, v_mps, section_length)\n",
    "                except ImpossibleState:\n",
    "                    # Transition not possible\n",
    "                    continue  # Skip this iteration\n",
    "\n",
    "                # As adding up speed and fuel usage is comparing apples and oranges, we have to prescale what we calculated \n",
    "                # so that the speed and efficiency factors have the effect we hope for.\n",
    "                cost_econ = E_Engine / 2500\n",
    "                cost_speed = time / 10\n",
    "\n",
    "                edge_weight = factor_time*cost_speed + factor_efficiency*cost_econ\n",
    "                G.add_edge(f\"x{x-1}_k{kold}\", node_name, weight=edge_weight, time=time, energy=E_Engine)\n",
    "\n",
    "\n",
    "\n",
    "x_end = len(config.index)-1\n",
    "k_end = int(np.ceil(config.loc[len(config.index)-1][\"vmin\"] / velocity_spacing))\n",
    "\n",
    "nodelist = nx.dijkstra_path(G, \"x0_k0\",f\"x{x_end}_k{k_end}\")\n",
    "\n",
    "outputlist = []\n",
    "for i in range(len(config.index)):\n",
    "     outputlist.append(G.nodes[nodelist[i]][\"v\"])\n",
    "\n",
    "output = pd.DataFrame(outputlist)\n",
    "output.rename(columns={0:\"Speed optimal\"}, inplace = True)\n",
    "output['distance'] = output.index * section_length\n",
    "\n",
    "total_time = 0\n",
    "for u, v in zip(nodelist[:-1], nodelist[1:]):\n",
    "    total_time += G.edges[u, v].get(\"time\", 0)\n",
    "print(\"Total time (seconds):\", total_time)\n",
    "\n",
    "\n",
    "total_energy = 0\n",
    "for u, v in zip(nodelist[:-1], nodelist[1:]):\n",
    "    total_energy += G.edges[u, v].get(\"energy\", 0)\n",
    "print(\"Total energy (joules):\", total_energy)\n",
    "\n",
    "weight_list = []\n",
    "attributelist = nx.get_edge_attributes(G,'weight')\n",
    "for edge in G.edges:\n",
    "    weight_list.append(attributelist[edge])\n",
    "\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos=coordinate_set_dict,\n",
    "    ax=ax,\n",
    "    with_labels=False,\n",
    "    node_color='blue',\n",
    "    edge_color='lightgrey',\n",
    "    node_size=2,\n",
    ")\n",
    "ax.plot(output['distance'], output['Speed optimal'], color='blue', linewidth=2, label='Optimal Speed')\n",
    "ax.set_xlabel('Distance [m]')\n",
    "ax.set_ylabel('Speed [km/h]')\n",
    "ax.set_title('Optimal Path')\n",
    "ax.axis('on')\n",
    "plt.tick_params(axis='both', which='both', bottom=True, left=True, labelbottom=True, labelleft=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
